{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is an algorithm used to train artificial neural networks by minimizing the error between the predicted output and the actual output.\n",
    "\n",
    "It works by propagating the error backwards through the network, adjusting the weights and biases at each layer to minimize the loss.\n",
    "\n",
    "The algorithm iteratively updates the weights and biases to minimize the loss, allowing the network to learn and improve its predictions.\n",
    "\n",
    "Backpropagation is a key component of many machine learning algorithms, including supervised learning and deep learning models.\n",
    "\n",
    "**Some important concepts**\n",
    "\n",
    "1. **Chain Rule**\n",
    "\n",
    "The chain rule is a fundamental concept in calculus that helps us compute the derivative of a composite function. In the context of Backpropagation, the chain rule is used to compute the derivative of the loss function with respect to the model's weights.\n",
    "\n",
    "Given a function f(x) = g(h(x)), where h(x) is another function, the chain rule states that:\n",
    "\n",
    "d/dx (f(x)) = d/dx (g(h(x))) * d/dx (h(x))\n",
    "\n",
    "In other words, the derivative of f(x) is the derivative of g(h(x)) times the derivative of h(x).\n",
    "\n",
    "2. **Forward Pass: Compute loss**\n",
    "\n",
    "The forward pass is the process of propagating the input data through the neural network to compute the output. During this pass, we compute the output of each layer, and the loss function is computed using the output and the target output.\n",
    "\n",
    "The forward pass involves the following steps:\n",
    "-   Compute the output of the input layer.\n",
    "-   Compute the output of each hidden layer.\n",
    "-   Compute the output of the output layer.\n",
    "-   Compute the loss function using the output and the target output.\n",
    "\n",
    "3. **Calculating local Gradients**\n",
    "\n",
    "After computing the loss function, we need to compute the local gradients, which are the derivatives of the loss function with respect to the model's weights. We do this by using the chain rule and the fact that the derivative of the loss function with respect to the output is the derivative of the loss function with respect to the weights times the derivative of the output with respect to the weights.\n",
    "\n",
    "To compute the local gradients, we follow these steps:\n",
    "\n",
    "- Compute the derivative of the loss function with respect to the output.\n",
    "- Compute the derivative of the output with respect to the weights.\n",
    "- Multiply the two derivatives to get the local gradients.\n",
    "\n",
    "4. **Backward Pass: Compute Gradient of Loss to Weights**\n",
    "\n",
    "The backward pass is the process of propagating the local gradients backwards through the neural network to compute the gradient of the loss function with respect to the model's weights.\n",
    "\n",
    "The backward pass involves the following steps:\n",
    "\n",
    "- Start with the local gradients computed in the previous step.\n",
    "- Compute the gradient of the loss function with respect to the output.\n",
    "- Compute the gradient of the output with respect to the weights.\n",
    "- Multiply the two gradients to get the gradient of the loss function with respect to the weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Given x = 1, y = 2, w = 1\n",
    "\n",
    "1. y^ = x * w\n",
    "2. s = y^ - y\n",
    "3. loss = s**2\n",
    "\n",
    "#### Forward pass\n",
    "\n",
    "1. y^ = x * w = 1 * 1 = 1\n",
    "2. s = y^ - y = 1 - 2 = -1 \n",
    "3. loss = s**2 = (-1)\\** 2 = 1\n",
    "\n",
    "#### Calculating Local gradients \n",
    "\n",
    "1. d(loss)/ ds = ds**2/ds = 2*s\n",
    "2. ds/dy^ = d(y^-y)/dy^ = 1 \n",
    "3. dy^/dw = d(x*w)/dw = x \n",
    "\n",
    "#### Backward pass\n",
    "\n",
    "1. d(loss)/dy^ = (d(loss)/ ds) * (ds/dy^) = 2 * s * 1 = -2\n",
    "2. d(loss)/dw = (d(loss)/dy^) * (dy^/dw) = -2 * x = -2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "w = torch.tensor(1.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y_hat = w*x\n",
    "loss = (y_hat - y) ** 2\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
